# 1 大模型推理运行流程

1. **提示词输入**： 用户输入的提示词经过预处理后，被转换为模型能够理解的向量表示（词嵌入）。每个词都会被映射到一个高维空间中的向量，这些向量包含了词的语义信息。
2. **特征提取**： 大模型通过多层的自注意力机制和前馈神经网络来提取输入提示词的特征。自注意力机制允许模型在处理每个词时，能够关注到输入序列中其他相关的词，从而捕捉到词与词之间的关系。例如，在处理句子 “The dog chased the cat” 时，模型可以通过自注意力机制确定 “dog” 是动作 “chased” 的执行者，“cat” 是动作的对象。
3. **生成预测**： 在提取了输入提示词的特征后，模型会根据这些特征预测下一个词的概率分布。它会在词汇表中为每个词计算一个概率值，表示生成该词的可能性大小。例如，对于提示词 “I want to”，模型可能会预测下一个词是 “go”、“eat”、“read” 等词的概率较高。
4. **采样或贪心搜索**： 为了生成下一个词，模型可以采用不同的策略。贪心搜索是选择概率最高的词作为生成的下一个词；而采样则是根据概率分布随机选择一个词。例如，贪心搜索在 “I want to” 之后可能会选择概率最高的 “go”，而采样则可能会以一定概率选择其他词。
5. **重复生成**： 模型会不断重复上述生成预测和选择下一个词的过程，直到达到预设的结束条件（如生成的文本长度达到指定值、遇到特定的结束标记等）。例如，持续生成词，最终得到 “I want to go to the park and play with my friends” 这样的完整文本。

# 2 KV缓存

在基于Transformer架构的大语言模型里，每一次前向传播都会涉及到注意力机制的计算，而注意力机制的核心操作是计算查询（Query）、键（Key）和值（Value）之间的相似度，进而根据相似度对值进行加权求和。在自回归生成任务中，模型会逐个生成token，除了最新生成的token外，之前的token对应的键和值在每次计算时都是相同的。因此，可以把这些已经计算好的键和值缓存起来，在后续的计算中直接使用，无需重复计算，以此提升推理速度。

这里以[llama源码](https://github.com/meta-llama/llama/blob/main/llama/model.py)为例进行说明：

1. **初始化缓存**：在推理开始之前，先初始化键和值的缓存。缓存的形状通常为`(batch_size, max_seq_len, num_heads, head_dim)`，这里的`max_seq_len`是最大序列长度。
2. **首次计算**：当输入第一个序列时，模型会计算该序列中所有token对应的键和值，并将其存储到缓存中。之后，利用这些键、值和查询来计算注意力分数和输出。
3. **后续计算**：当输入新的token时，模型仅计算该新token对应的键和值，然后把这些新的键和值追加到缓存里。接着，使用缓存中的所有键和值以及新token对应的查询来计算注意力分数和输出。

# 3 分词器

以[llama中的分词器](https://github.com/meta-llama/llama/blob/main/llama/tokenizer.py)为例，Tokenizer可以把文本编码为词元 ID 列表，而词元 ID 就是大模型的输入。也可以将词元 ID 列表解码回文本，也就是可以把大模型的输出解码为自然语言。

所以分析器可以看做是自然语言和大模型之间的桥梁。

# 4 BEP分词算法

BPE算法的核心思想是通过迭代地合并最频繁出现的字节对（字符对或子词对），逐步构建出更大的子词单元，直到达到预设的合并次数或词汇表大小。初始时，将文本拆分成单个字符作为基本单元，然后不断合并最频繁出现的相邻单元，形成新的单元，这个过程持续进行，直到满足停止条件。

1. **初始化**：将训练文本中的每个字符作为一个独立的词元，构建初始词汇表。同时，将训练文本拆分成字符序列。
2. **统计字节对频率**：遍历训练文本，统计所有相邻字节对的出现频率。
3. **合并最频繁的字节对**：选择出现频率最高的字节对，将其合并成一个新的词元，并添加到词汇表中。同时，在训练文本中更新所有出现该字节对的地方，用新的词元替换。
4. **重复步骤2和3**：不断重复统计字节对频率和合并最频繁字节对的过程，直到达到预设的合并次数或词汇表大小。






